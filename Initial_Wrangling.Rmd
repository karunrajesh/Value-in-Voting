---
title: "R Notebook"
output:
  html_document:
    df_print: paged
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Cmd+Shift+Enter*. 

```{r include=FALSE}
library(tidyverse)
library(ggplot2)
library(dplyr)
library(stringr)
library(tabulizer)
library(rvest)
library(pROC)
```

## Introduction and Overview

Voting has become an increasingly important topic, especially in the United States. Studies have shown that those ages 18 to 29 have consistently voted at lower levels, over 15% lower, than general election turnout. This can be attributed to many reasons. Habit formation in initial voting, opportunity cost of voting, and alternative participation like protesting. Another reason is that the value of voting is not apparent. Someone voting in California, where it will almost assuredly vote democratic, may not see a point to participating in a process where their vote is drowned in a sea of trillions of others. 

```{r echo=FALSE, results='hold', fig.cap="Voter Turnout by Age"}
knitr::include_graphics("Data/turnout.png")
```


Without this knowledge of value, people will not understand the impact they can have and continue to be unseen in elections. To increase voter turnout, it is imperative that the value of one’s vote becomes clear at every level of election around them. For instance, the 2020 election has been one of the most heavily funded elections with the most voter turnout in U.S. history. The record number of civil engagements highlighted the importance of this election. We have seen many different organizations like the NFL, NBA, Facebook, and others continuously push voting ad-campaigns like never before. It immediately made us think about the value of our singular votes and how much change we can affect with just this one action. We also wanted to provide an alternative view to the case of voting, besides the worn out “civic duty” argument, to peers around us who did not see reason in voting.
 
However, just how much is your vote worth? How much does a vote actually matter in terms of its economic impact? These questions could be critical in future campaign in promoting voting by directly demonstrating in a quantitative fashion how much impact each vote makes. 
 
In order to begin answering these questions, we obtained a detailed breakdown of data on voter turnout, how much money each administration has spent on relevant sectors such as education, agriculture, military dense etc. and the possibility of a single vote overturning election results by state from past US presidential elections. We then can calculate the monetary value of each vote for particular sectors of interests. Further we hope to use these datasets to draw inferences for the values in a single vote in 2024 election.



## Related Work

While civil engagement has been a topic of interest for an extended period of time, when searching literature bases, we found that most of the studies on values in voting are related to communal moral values and political social values, instead of our focus on physical monetary values. 
 
The value of a singular vote is often discussed in many commentaries and literature. For example, the article titled “The Importance of Voting” (https://www.castleton.edu/news-media/article/the-importance-of-voting/) discussed how many political scientists and civic educators express the idea that voting is important without explaining why that is the case. It also discussed this misconception that many Americans have about their votes appearing unimportant and unlikely to make a difference. It presents the striking statistics that for 2016 presidential election, the entire outcome hinged on approximately 78,000 votes, or less than 0.06% of the total votes cast. More so, in 2017, the 94th District of the Virginia House of Delegates, wherein 11,608 ballots were cast, resulted in a tie, and the winner was chosen by randomly drawing a name from a hat. These tight races all highlight the importance of even singular votes and similar examples vastly exceed just the two mentioned above. 
 
On the other hand, research in voter turnout has shown that the U.S. trails consistently most developed countries. According to Pew Research (https://www.pewresearch.org/fact-tank/2020/11/03/in-past-elections-u-s-trailed-most-developed-countries-in-voter-turnout/), in 2016, the U.S. had a 55.7% VAP turnout which puts the U.S. behind most of its peers in the Organization for Economic Cooperation and Development, most of whose members are highly developed democratic states. Additionally, turnout rate for people of color was also a concerned with a significant decline in the 2016 election, according to Pew Research (https://www.pewresearch.org/fact-tank/2017/05/12/black-voter-turnout-fell-in-2016-even-as-a-record-number-of-americans-cast-ballots/). These studies suggest that conveying the monetary values of civic engagement, and targeted support for people of color and underrepresented minority are important to facilitate higher voter turnout and could be highly beneficial to the democratic process.

## Initial Questions

Voting has become an increasingly important topic, especially in the United States. Those ages 18 to 29 have consistently voted at lower levels, over 15% lower, than general election turnout. This can be attributed to many reasons. Habit formation in initial voting, opportunity cost of voting, and alternative participation like protesting. Another reason is that the value of voting is not apparent. Someone voting in California, where it will almost assuredly vote democratic, may not see a point to participating in a process where their vote is drowned in a sea of trillions of others. Without this knowledge of value, people will not understand the impact they can have and continue to be unseen in elections. To increase voter turnout, it is imperative that the value of one’s vote becomes clear at every level of election around them.

On a more personal note, we have seen many different organizations like the NFL, NBA, Facebook, and others continuously push voting ad-campaigns like never before. It immediately made us think about the value of our singular votes and how much change we can effect with just that one action. We also wanted to provide an alternative view to the case of voting, besides the worn out “civic duty” argument, to peers around us who did not see reason in voting.

Originally, we wanted to find the value of a vote at different election levels (presidential, senate, etc.); however, we found this to not be as effective in communicating the value of a vote. Thus, we focused solely on the presidential election level. 

## Data Sources:

In this project, we use three main data sources, supplemented by other various datasets that have been web scraped. These sources allowed us to quantify the value of a single vote over the past 40 years of presidential elections. 

#### MIT Election Lab

The MIT Election Lab is an online repository that stores past years (up to 1976) of election data at various levels (senate, presidential, etc.) These datasets are continuously updated each year, with new elections. Data in this repository is thoroughly cleaned, such that users can perform their own analysis directly after downloading. This dataset was used to obtain the voter turnout for each state at both the presidential and senate level (1976 - 2016). It lists not only the winner’s vote total for each state, but also other candidates' vote totals as well. From this dataset, we were able to derive voter turnout by state, margins of victory by state, and flipping of states between elections from 1976 to 2016. 

```{r warning=FALSE}
presidential <- read.csv("Data/1976-2016-president.csv")
senate <- read.csv("Data/1976-2018-senate.csv")

head(presidential)
```

We then took the presidential and senate dataset and subset it into various other datasets. The dataframe below holds the voter turnout over time, aggregated by state and year. 

```{r warning=FALSE}
#Voter Turnout Over Time
senate_turnout <- senate %>%
  group_by(year, state) %>%
  filter(row_number()==1) %>%
  dplyr::select(year, state, totalvotes)
presidential_turnout <- presidential %>%
  group_by(year, state) %>%
  filter(row_number()==1) %>%
  dplyr::select(year, state, totalvotes)
```

The dataframe below holds the margin of victory for each presidential race, aggregated by state and year. 

```{r warning=FALSE}
#Margins of Victory
margin_pres <- presidential %>%
  group_by(year,state) %>%
  arrange(desc(candidatevotes), .by_group = TRUE) %>%
  filter(row_number() %in% c(1,2)) %>%
  dplyr::select(year, state, candidate, candidatevotes, totalvotes, party) %>%
  summarize(margin = mean((candidatevotes[1]-candidatevotes[2])/totalvotes), party = party[1])

margin_sen <- senate %>%
  group_by(year,state) %>%
  arrange(desc(candidatevotes), .by_group = TRUE) %>% 
  filter(row_number() %in% c(1,2)) %>%
  dplyr::select(year, state, candidate, candidatevotes, totalvotes, party) %>%
  summarize(margin = mean((candidatevotes[1]-candidatevotes[2])/totalvotes), party = party[1])
```

The dataframe below holds the party that won for each presidential race, aggregated by state and year. 

```{r warning=FALSE}
# Switching Parties
parties_pres <- presidential %>%
  group_by(year,state) %>%
  arrange(desc(candidatevotes), .by_group = TRUE) %>%
  filter(row_number() == 1) %>%
  dplyr::select(year, state, candidate, party)
parties_sen <- senate %>%
  group_by(year,state) %>%
  arrange(desc(candidatevotes), .by_group = TRUE) %>%
  filter(row_number() == 1) %>%
  dplyr::select(year, state, candidate, party)

```

We also used an analysis dataset published from this repository. This dataset consisted of the voter demographics in each county of the United States for the 2012 and 2016 presidential elections. It included voting breakdowns by race, gender, age, level of education, level of employment, and environment. We used this to understand more about the voting population, as well as, predict the value of a vote. 

```{r warning=FALSE}
context <- read.csv("Data/election-context-2018.csv")
head(context)
```

We then aggregated this demographic data by state, as it was currently separated by county.

```{r}
context_state<- context %>%
  group_by(state) %>%
  dplyr::select(-county) %>%
  summarise_each(funs(mean))
```

#### White House Office of Management and Budget

The White House’s Office of Management and Budget website contains historical tables that detail spending over several different categories (discretionary, deficits, agency, etc.) From this site, we were able to download the discretionary spending estimates over each year, starting from 1976, to an estimated amount in 2025. These estimates were split by agencies (Department of Defense, Department of Education, Department of Transportation, etc.) This was downloaded as a csv file, which was then cleaned in R. 

We aggregated this data over each president’s term to get the total discretionary spending per term. This was then used to find the monetary value of one person’s vote in a given state for the given election year. 

```{r warning=FALSE}
# Contains the mandatory federal spending per year per agency
budg_all <- read.csv("Data/budget_amount.csv")
budg_all <- data.frame(lapply(budg_all, as.character), stringsAsFactors=FALSE)
colnames(budg_all) = budg_all[2,]
rownames(budg_all) = budg_all[,1]
budg_all = budg_all[-c(1,2,33,34),-1]
cols <- colnames(budg_all)
rows <- rownames(budg_all)
budg = as.data.frame(lapply(budg_all, function(x) as.numeric(gsub(",","",x))))
colnames(budg) <- cols
rownames(budg) <- rows
budg <- budg[,-c(1,2)]
budg_admin <- as.data.frame(sapply(seq(1,dim(budg)[2]-1,by=4),function(i) rowSums(budg[,i:(i+3)], na.rm=TRUE)))
colnames(budg_admin) <- c(as.numeric(colnames(budg[,seq(1,dim(budg)[2]-5,by=4)])) - 1, 2020)

```

#### United States Election Project

The United States Election Project is an information source that houses election data for each state over the past 50 years of elections. This source was mainly used to obtain the voter turnout as a percentage of the total eligible voters. This was combined with two other sources to obtain the full voter turnout per state from the 1976 to 2016 presidential election. 

Show missing 1976 and 2016 columns and then explain later how it was filled in with web scraping

```{r Turnout}
location <- 'https://sos.nh.gov/media/ni0j3zb5/1976-2016-presidental-election-highest-office-divided-by-vap.pdf'
out <- extract_tables(location)
df <- out[[2]]
final <- as.data.frame(df)
final <- final[,-c(1,4,7,10,13,16)]
final <- data.frame(lapply(final, as.character), stringsAsFactors=FALSE)
colnames(final) <- final[1,]
final <- final[-1,]

final <- final[,c(11,12)]
final <- final %>%
  filter(`2016` != "United States")
final$X <- 2016
final <- final[,c(3,1,2)]
colnames(final) <- c("Year", "State", "Turnout")
final$Year <- as.numeric(final$Year)

write.csv(final,"Data/2016_Turnout.csv",row.names=F)


check <- read.csv("Data/Turnout.csv")
check <- check %>%
  filter(X %in% seq(1976, 2016, by = 4)) %>%
  filter(State != "United States") %>%
  select(X, State, X.4)
colnames(check) <- c("Year", "State", "Turnout")
check$Year <- as.numeric(as.character(check$Year))
total_turnout <- rbind(final,check)
total_turnout$Turnout <- as.numeric(gsub("%","",total_turnout$Turnout))

old_turnout <- data.frame(State = c("Minnesota", "Utah", "North Dakota", "Wisconsin", "South Dakota", "Maine", "Montana", "Iowa", "Conneticut", "Massachusetts", "Oregon", "Idaho", "Indiana", "Washington", "Rhode Island", "Illinois", "Colorado","Kansas", "Michigan","Wyoming","New Jersey","Missouri", "New Hampshire","Delaware","West Virginia", "Nebraska","Vermont","Ohio","Oklahoma", "Pennsylvania", "New Mexico","Arkansas","New York", "California","Maryland","Florida","Louisiansa", "Tennessee","Alaska","Mississippi","Kentucky", "Virginia","Hawaii","Texas","Alabama","Arizona","Nevada","North Carolina","Georgia","South Carolina", "District of Columbia"), Turnout = c(71.5, 68.4, 67.2, 66.5, 64.1, 63.6, 63.3, 
                       63.1, 62.7, 61.6, 61.3, 60.6, 60.1, 59.8,
                       59.6, 59.4, 58.8, 58.8, 58.8, 58.5, 57.7, 
                       57.3, 57.2, 57.2, 57.1, 56.2, 55.7, 55.1, 
                       54.8, 54.1, 53.4, 51.1, 50.6, 50.4, 49.3,
                       49.1, 48.7, 48.6, 48.0, 48.0, 47.9, 46.9, 46.6, 
                       46.3, 46.3, 46.1, 44.1, 42.9, 42.0, 40.2,
                       32.2))
old_turnout$Year <- 1976
old_turnout <- old_turnout[,c(3,1,2)]

write.csv(old_turnout,"Data/1976_Turnout.csv",row.names=F)

total_turnout <- rbind(total_turnout, old_turnout)


# how im reading turnout in shiny app, 
turnout <- read.csv("Data/Turnout.csv") %>%
  filter(X %in% seq(1976, 2016, by = 4)) %>%
  select(X, State, X.4) %>% 
  rename(year=X,state=State, turnout=X.4)
turnout_1976 <- read.csv("Data/1976_Turnout.csv")
turnout_2016 <- read.csv("Data/2016_Turnout.csv") %>% mutate(State=recode(State,!!!c("Dist. of Columbia"="District of Columbia")))
```


#### Andrew Gelman

Andrew Gelman is a professor of statistics and political science at Columbia University. He was one of the first people to publish studies that utilized statistical models to quantify the value of a vote. His work in the field motivated much of this work, and also served as a key data source. He published an article on Slate (https://slate.com/news-and-politics/2016/11/here-are-the-chances-your-vote-matters.html) that listed the probability of a single vote being decisive in the 2012 election.

```{r echo=FALSE, results='hold', fig.cap="Adapted from Gelman's Slate Article"}
knitr::include_graphics("Data/Gelman.jpg")
```

These were separated by states, with battleground states having higher values, Wisconsin with a 1 in 2 million chance, and dominant states having lower values, Oklahoma with a 1 in 30 billion chance. We used the probability of influencing an election, along with the discretionary budget data, to find the monetary value of a vote based on state and year. It should be noted that we do not replicate Gelman’s statistical models over each election year and utilize just one election year for our analysis. 

```{r}
#Adapted from: https://slate.com/news-and-politics/2016/11/here-are-the-chances-your-vote-matters.html
influence = data.frame(state = c(state.abb), 
                       chance = c(1/7e9,1/100e6,1/40e6, 1/3e9,
                                 1/7e9,1/1e6, 1/40e6, 1/300e6,
                                 1/3e6, 1/60e6, 1/1e9, 1/10e9,
                                 1/1e9, 1/3e9, 1/30e6, 1/1e9,
                                 1/8e9, 1/5e9, 1/5e6, 1/10e9,
                                 1/4e9, 1/3e6, 1/10e6, 1/1e9,
                                 1/1e9, 1/2e9, 1/4e9, 1/2e6,
                                 1/1e6, 1/400e6, 1/3e6, 1/3e9,
                                 1/3e6, 1/4e9, 1/20e6, 1/30e9,
                                 1/40e6, 1/2e6, 1/30e6, 1/100e6,
                                 1/3e9, 1/3e9, 1/1e9, 1/2e9,
                                 1/10e9, 1/10e6, 1/200e6, 1/9e9,
                                 1/2e6, 1/30e9))


va <- influence %>%
  filter(state == "VA")
va_example <- budg_admin * va$chance * 1e6
```


#### Web Scraping

We web scraped several other datasets to fill in gaps found throughout the process. One of these was the voter turnout from 2016. This was obtained by scraping a table from sos.nh.gov that contained voter turnout in the year 2016. Various other datasets that showed differences in administration’s proposed budgets were also web scraped from https://www.thebalance.com/ . This site contains the breakdown of discretionary spending per year, and contains tables comparing President Trump to President Obama’s (2012) proposed budgets. We also scraped data from govinfo.gov that contained data comparing President Bush’s and President Obama’s (2008) proposed budget. 


```{r Web Scraping}
# Web Scraping, Potentially not needed
clean_table <- function(df){
  rownames(df) <- df[,1]
  df[,1] <- NULL
  colnames(df) <- df[1,]
  df <- df[-1,]
} 
get_table <- function(url, i){
  h1 <- read_html(url)
  try <- h1 %>% html_nodes("table") %>% .[i]
  budget <- try %>% html_table %>% .[[1]] 
  budget <- clean_table(budget)
  return(budget)
}

budget_2020 <- get_table("https://www.thebalance.com/fy-2020-federal-budget-summary-of-revenue-and-spending-4797868", 1)



turl <- "https://www.thebalance.com/how-trump-amended-obama-budget-4128986"
h   <- read_html(turl)
tab <- h %>% html_nodes("table")
try <- h %>% html_nodes("table") %>% .[1]
budget_comp <- try %>% html_table %>% .[[1]] 

url_2013 <- "https://www.thebalance.com/fy-2013-u-s-federal-budget-and-spending-3306319"
h1 <- read_html(url_2013)
try_2013 <- h1 %>% html_nodes("table") %>% .[2]
budget_2013 <- try_2013 %>% html_table %>% .[[1]] 


budget_2013 <- clean_table(budget_2013)
budget_comp <- clean_table(budget_comp)

# Location of WARN notice pdf file
location <- 'https://www.govinfo.gov/content/pkg/BUDGET-2009-BUD/pdf/BUDGET-2009-BUD-31.pdf'

# Extract the table
out <- extract_tables(location)
df <- out[[4]]
final <- as.data.frame(df)
final <- final[-c(1:5),]
final[,c(5:7)] <- NULL
rows <- as.character(final[,1])
final[,1] <- NULL
colnames(final) <- c("2001", "2008", "2009")
final_f <- mutate_all(final, function(x) as.numeric(as.character(x)))
rownames(final_f) = str_match(rows, "^(.*?)\\.")[,2]
```

## Exploratory Analysis

We broke down our exploratory analysis into five main portions: 

* Demographics

* Voting Trends Over Time

    + Voter Turnout
    + Voting Margin of Victory
    + Election Result by State

* Chance of Influencing a Single Election

* Discretionary Budget Breakdown

* Value of a Vote


#### Demographics


```{r Demographics}

# Demographics, 2016 Election, show as table 
demo <- context_state[,-c(2:22)]
demo <- demo %>%
  filter(!state %in% c("Missouri", "Virginia", "South Dakota"))
head(demo)

```

#### Voting Trends Over Time

We first wanted to explore trends in voter turnout over time. The plots below show the overall voter turnout from 1976 to 2016, as well as state level voter turnout. In our shiny app, we allow the user to select up to six states to display the voter turnout. 

```{r InitalPlots, echo=FALSE, message=FALSE, warning=FALSE}
overall_turnout <- presidential_turnout %>%
  group_by(year) %>%
  summarize(total = sum(totalvotes))

# Overall Voter Turnout
overall_turnout %>%
  ggplot(aes(year,total)) + 
  geom_line() +
  geom_point()
```

```{r }
#Voter turnout by State, could have user select state
presidential_turnout %>%
  filter(state == "Virginia") %>%
  ggplot(aes(year,totalvotes)) + 
  geom_line() +
  geom_point()

```

What we realized is that this only communicated the number of total people that voted from a given state. This would not be an effective way to show voter turnout especially when comparing a state like California with a high population to Oklahoma with a much lower population. This is why we pulled the voter turnout percentage from each state using the dataset provided by the United States Election Project, as talked about above.

```{r }
# Use the turnout percentage
#Voter turnout by State, could have user select state
turnout <- read.csv("Data/Turnout.csv") %>%
  filter(X %in% seq(1976, 2016, by = 4)) %>%
  select(X, State, X.4) %>% 
  rename(year=X,state=State, turnout=X.4)
turnout_1976 <- read.csv("Data/1976_Turnout.csv")
turnout_2016 <- read.csv("Data/2016_Turnout.csv") %>% mutate(State=recode(State,!!!c("Dist. of Columbia"="District of Columbia")))
presidential_turnout <- turnout_1976 %>% 
  rename(year=Year,state=State,turnout=Turnout) %>% rbind(turnout) %>% 
  rbind(turnout_2016 %>% rename(year=Year,state=State,turnout=Turnout)) %>%
  mutate(year = as.numeric(as.character(year)),turnout=as.numeric(str_replace(turnout,"%",""))) %>%
  filter(state !='United States')

presidential_turnout %>%
  filter(state == "Virginia") %>%
  ggplot(aes(year,turnout)) + 
  geom_line() +
  geom_point()

```


We then looked at the margin of victory for each state. With this, we hoped to show how some states, such as Oklahoma and Wyoming were strongholds that had large margins of victory, whereas, other states such as New Hampshire were closer in margin. We also wanted to see what trends existed in these margins over time. What have the margins been like over time for a battleground state as compared to a stronghold state. 

```{r }
# Margin of Victory, by state
margin_pres %>% 
  filter(state %in% c("Virginia")) %>%
  ggplot(aes(year,margin)) + 
  geom_line() +
  geom_point(aes(colour = party), size = 5) + 
  scale_color_manual(values=c("blue", "red"))
```

What we realized was that we could use a heatmap to better communicate the overall trends in turnout, margins of victory, and the result of the election for each state. This was the method we used on our shiny application. Users would be able to pick the year of choice and whether they wanted to display the turnout, margin of victory, or result for each state. 

```{r echo=FALSE}
us_states <- map_data("state") %>% mutate(state=region)

presidential_turnout_w_map <- presidential_turnout %>%
  mutate(state = tolower(state)) %>% left_join(us_states,by='state')
margin_pres_w_map <- margin_pres %>%
  mutate(state = tolower(state)) %>% left_join(us_states,by='state')
parties_pres_w_map <- parties_pres %>%
  ungroup() %>%
  mutate(state = tolower(state)) %>% left_join(us_states,by='state')

color_scale_min <- c('Margins'= min(margin_pres$margin),'Voter Turnout'=min(presidential_turnout$turnout))
color_scale_max <- c('Margins'= max(margin_pres$margin),'Voter Turnout'=max(presidential_turnout$turnout))

mapdata <- presidential_turnout_w_map %>% filter(year==2016) %>% mutate(fillcolor=turnout)

p<- mapdata %>% ggplot(aes(x = long, y = lat, fill = fillcolor, group = group)) + 
      geom_polygon(color = "white") + 
      coord_fixed(1.3) +
      ggtitle('Voter Turnout') +
      theme(panel.grid.major = element_blank(), 
            panel.background = element_blank(),
            axis.title = element_blank(), 
            axis.text = element_blank(),
            axis.ticks = element_blank(),
            text = element_text(size=15),
            legend.position="bottom",
            plot.title = element_text(hjust = 0.5)
      )
p <- p + scale_fill_distiller(name='Voter Turnout',
                                  palette="Spectral",
                                  limits= c(color_scale_min['Voter Turnout'],color_scale_max['Voter Turnout'])
             ) + theme(legend.key.width = unit(6,"lines"))
p
```

```{r echo=FALSE}
mapdata<- margin_pres_w_map %>% filter(year==2016) %>% mutate(fillcolor=margin)

p<- mapdata %>% ggplot(aes(x = long, y = lat, fill = fillcolor, group = group)) + 
      geom_polygon(color = "white") + 
      coord_fixed(1.3) +
      ggtitle('Margin of Victory in 2016 Presidential Election') +
      theme(panel.grid.major = element_blank(), 
            panel.background = element_blank(),
            axis.title = element_blank(), 
            axis.text = element_blank(),
            axis.ticks = element_blank(),
            text = element_text(size=15),
            legend.position="bottom",
            plot.title = element_text(hjust = 0.5)
      )
p <- p + scale_fill_distiller(name="Margins",
                                     trans='log10',
                                     palette="Spectral",
                                     limits= c(color_scale_min["Margins"],color_scale_max["Margins"])
       ) + theme(legend.key.width = unit(6,"lines"))

p

```

```{r echo=FALSE}
mapdata <- parties_pres_w_map %>% filter(year==2016) %>% mutate(fillcolor=str_to_title(party))

p<- mapdata %>% ggplot(aes(x = long, y = lat, fill = fillcolor, group = group)) + 
      geom_polygon(color = "white") + 
      coord_fixed(1.3) +
      ggtitle("2016 Presidential Election Results") +
      theme(panel.grid.major = element_blank(), 
            panel.background = element_blank(),
            axis.title = element_blank(), 
            axis.text = element_blank(),
            axis.ticks = element_blank(),
            text = element_text(size=15),
            legend.position="bottom",
            plot.title = element_text(hjust = 0.5)
      ) 

p <- p + scale_fill_manual(name='Party',values=c("Democrat"="blue","Republican"="red","Democratic-Farmer-Labor"="darkgreen"))
p
```

#### Chance of Influencing a Single Election

Andrew Gelman published multiple papers and articles, as detailed above, that used statistical models to find the chance a single vote would swing an election. This served as our main launching point for finding the value in a vote. By equating this probability to each person's individual vote, we can start to answer how much value is in a vote. For instance, in the graph below we can see that a singular vote in Nevada (5 in 10 million) has a much higher chance of swinging an election as compared to Wyoming, which is almost 0 (1 in 30 billion). Thus the value of that singular vote in Nevada can be considered to be much higher than in Wyoming. We then extend this idea into our next section where we discuss the monetary value of a vote. 

```{r }
# Chance of influencing the election, based on 2012 results, # NEEDS INFLUENCE TABLE BELOW
# Show 5-6 states
influence$chance <- as.numeric(influence$chance)
influence %>%
  filter(state %in% c("AL", "NV", "NC", "VA", "WY")) %>%
  ggplot(aes(state, chance, fill = state)) +
  geom_bar(stat="identity") 

```

#### Discretionary Budget Breakdown

When understanding the value of a vote, we thought to first find out the money spent during a president's term. This comes in the form of discretionary spending, which is allocated by a plan proposed by the president that is then approved. The reasoning behind this is so users could better understand where their vote was going. For instance, in 1976 presidency, the legislative branch received a total of \$4,543,000,000, while the Judicial Branch only received \$ 1,996,000,000 in comparison. From this, we can see the breakdown of how each president allocated their discretionary spending, and trends of this spending over time. Below is a table showing the discretionary spending for each agency from 1976 to an estimated amount during the 2020 presidency. 

``` {r }
head(budg_admin)
```

From the graph below, we can see that Department of Education's allocated budget increased much faster compared to the Department of Transportation's. We can also see a sharp increase in the Department of Education in 2008, or President Obama's first term. 

```{r}
gov_agencies <- c("Department of Education", "Department of Transportation")
line_data <- budg_admin %>% 
  .[gov_agencies,] %>% 
  tibble::rownames_to_column("agency") %>% 
  gather(year,budget,-agency) %>%
  mutate(year = as.numeric(year))

line_data %>%
  ggplot(aes(x=as.numeric(year), y=budget, color=agency)) + 
  geom_line() +
  geom_point() +
  xlab("Year") + 
  ylab("Discretionary Spending Amount") +
  ggtitle('Discretionary Spending Amount Over Time') +
  theme(legend.position="bottom",
        # center title
        plot.title = element_text(hjust = 0.5))
```

#### Value of a Vote

We now move into the actual monetary value of a vote. Given the discretionary spending during a president's term and the probability of a singular vote in a given state, the expected value of the vote is equal to:

$$E[Value] = Discretionary Spending * P(Vote|State)$$

We can then plot this value for each presidency term from 1976 to 2016 for a given state and agency to find the expected value of a vote. As can be seen in the plot below, the expected value of a single vote in Virginia for the 2008 presidency was greater than $35,000 towards the Department of Education. This is an incredibly large amount a single vote holds and shows just how much power a single vote can contribute. 

```{r }
gov_agencies <- c("Department of Education", "Department of Transportation")
chosen_state <- "VA"
influence %>% filter(state == chosen_state)
line_data <- (influence %>% filter(state == chosen_state) %>% 
      .$chance * budg_admin *1e6) %>% 
      .[gov_agencies,] %>% 
      tibble::rownames_to_column("agency") %>%
      gather(year,budget,-agency)


line_data %>%
  ggplot(aes(x=as.numeric(year), y=budget, color=agency)) + 
  geom_line() +
  geom_point() +
  xlab("Year") + 
  ylab("Value of Vote (in dollars)") +
  ggtitle(sprintf('Discretionary Spending Amount Over Time for %s',chosen_state)) +
  theme(legend.position="bottom",
        # center title
        plot.title = element_text(hjust = 0.5))
```


## ML Analysis - 2024 Competitive State prediction

We want to forecast which state will become a competitive state in the upcoming election. A state is defined to be competitive when the winning candidate only wins 1% or less of the total number of votes in that state. We start by visually assessing the relationship between vote margins and voter turnout each state using a bubble plot, where the size of the bubble is represented by the number of electoral votes that state have. We webscraped the number of electoral votes for each state historically since 1976 to 2016. 

```{r ElectoralVotes}
### Relationship between  over Time
# Webcrape historical number of electoral votes by state by year
url <- "https://www.270towin.com/state-electoral-vote-history/"
electoral_votes <- read_html(url) %>% html_nodes("table") %>% html_table %>% .[[1]] 
electoral_votes <- electoral_votes %>% gather(year,nvotes,-1)
names(electoral_votes) <- c('state','year','nvotes')
electoral_votes <- electoral_votes %>% mutate(year=as.numeric(year)) %>% filter(year %in% seq(1976,2016,4))
write.csv(electoral_votes,'Data/Electoral_Votes_by_State.csv',row.names=F)

```

Now we have all data necessary for our bubble chart:

```{r BubblePlot,out.width='100%',out.height='100%'}
### Relationship between  over Time
electoral_votes <- read.csv('Data/Electoral_Votes_by_State.csv')
pres_turnout_and_margin <- presidential_turnout %>% 
  left_join(margin_pres,by=c('state','year')) %>%
  left_join(electoral_votes,by=c('state','year'))

year_scatter <- 2016
scatter_data<- pres_turnout_and_margin %>% filter(year==year_scatter) %>% 
                           left_join(influence,by='state') %>% 
                           mutate(party=str_to_title(party))

color_scale_min <- c('Margins'= min(margin_pres$margin),'Voter Turnout'=min(presidential_turnout$turnout))
color_scale_max <- c('Margins'= max(margin_pres$margin),'Voter Turnout'=max(presidential_turnout$turnout))


p <- scatter_data %>%
  ggplot(aes(margin, turnout, color = party, size=nvotes)) +
  geom_point(alpha = 0.5) +
  xlab("Margin") +
  ylab("Turnout") +
  scale_x_log10(limits=c(color_scale_min['Margins'],color_scale_max['Margins'])) +
  ylim(c(color_scale_min['Voter Turnout'],color_scale_max['Voter Turnout'])) +
  # Set x and y axis limits to keep the same for each year
  # Make the legend titles nicer
  scale_color_manual(name='Winning Party',values=c("Democrat"="blue","Republican"="red","Democratic-Farmer-Labor"="darkgreen")) +
  scale_size_continuous(name = "# of Electoral Votes",range=c(2,15)) +
  
  # Change the title of the plot for each year
  # Returns a character vector containing a formatted combination 
  # of text and variable values
  ggtitle(sprintf("Turnout vs Margin in %d Presidential Election", year_scatter)) +
  theme_bw() 

p
```
For 2016 presidential election, there is definite negative correlation between voters turnout and vote margin results for each state. We will now fit a logistic regression model with linear covariates on historical data for which states were competitive in past presidential elections. The covariates in our regression include US Census variables for these states as well as the election year and voters turnout data for these states.

```{r Regression, fig.width=5,fig.asp = 0.95}
#Setting up Data:
# Calc expected value of vote by state by sector
full_set <- budg_admin %>% 
  tibble::rownames_to_column("sector") %>% 
  gather(year,budget,-sector) %>% 
  left_join(influence,by=character()) %>% #cross join with chance of flipping
  mutate(value=chance*budget) %>% select(-c("chance","budget")) # expected budget value

# Put in Census variables as features
state_name_to_abb <- setNames(state.name,state.abb)
full_set <- full_set %>% 
  mutate(state=recode(state,!!!state_name_to_abb)) %>%
  left_join(demo,by='state')

# Put in vote margins
full_set <- full_set %>% mutate(year=as.numeric(year)) %>%
  left_join(margin_pres,by=c('year','state')) %>% 
  left_join(presidential_turnout,by=c('year','state'))

# make swing state
full_set <- full_set %>% mutate(swing_state = margin<= .01)

#plan A predict swing state in 2024
#swing state: vote_margin < .0001
# predictors: census variables, year (numeric), electoral votes
set_in <- full_set %>% select(-sector, - value, -margin, - party, - white_pct) %>% unique()
swing_clf<- glm(swing_state ~ . - swing_state- state,family=binomial(),data=set_in)
summary(swing_clf)

roc.1 <- roc(drop_na(set_in)$swing_state, fitted(swing_clf))
#fitted_vals<-fitted(swing_clf)
#plot(roc.1,col='red',xlim=c(1,0))
roc_df <- as.data.frame(cbind(roc.1$sensitivities,roc.1$specificities))
names(roc_df) <- c('sensitivity','specificity')
roc_df <- roc_df %>% arrange(desc(sensitivity),desc(specificity))

roc_df %>% 
  ggplot(aes(specificity,sensitivity)) + 
  geom_line(color='darkred') + 
  geom_abline(aes(slope=1,intercept=1),linetype='dashed') + 
  xlim(1,0) +
  ggtitle("Reciever Operating Characteristic Curve") +
  xlab("Sensitivity") + 
  ylab("Specificity")
print(auc(roc.1))

# plan B: value of your vote based on year and sector in 2024
# mod <- lm(value ~ year + sector) # plan B
# mod <- lm(value ~ . - value - state - nonwhite_pct, data = drop_na(full_set))
# summary(mod)
#residuals(mod)
```
We achieve 0.8 for the AUC of our ROC Curve, which means our classifier fitted pretty well onto the dataset. Upon looking at the summary of our regression output, only voters turnout seems to be a significant predictor in predicting if the state would be competitive or not at the 0.05 level. The US Census variables were not as helpful and neither was the year. Now we can forecast into 2024 on which state will be competitive.

```{r election forcast}

# use latest turnout
predict_inputs <- set_in %>% 
  filter(year==2016) %>% 
  unique() %>%
  add_column(year=2024) %>% 
  drop_na()

predictions_2024 <- predict(swing_clf, newdata = predict_inputs, type="response")
names(predictions_2024) <- predict_inputs$state
predictions_2024 <- as.data.frame(predictions_2024) %>% tibble::rownames_to_column('state') %>% rename(swing_prob=predictions_2024)

write.csv(predictions_2024,'Data/2024_swing_state_predictions.csv')

```


```{r, fig.width=10}
mapdata <- predictions_2024 %>% 
 mutate(state = tolower(state)) %>% 
  left_join(us_states,by='state')
 

p<- mapdata %>% ggplot(aes(x = long, y = lat, fill = swing_prob, group = group)) + 
      geom_polygon(color = "white") + 
      coord_fixed(1.3) +
      ggtitle("Which State will be a Competitive in 2024 Election?") +
      theme(panel.grid.major = element_blank(), 
            panel.background = element_blank(),
            axis.title = element_blank(), 
            axis.text = element_blank(),
            axis.ticks = element_blank(),
            text = element_text(size=10),
            legend.position="bottom",
            plot.title = element_text(hjust = 0.5),
            legend.key.width = unit(4,"lines")
      ) +
    scale_fill_distiller(name='Probability of Being Competitive',
                                  palette="Spectral")
p
```

## Final Analysis

